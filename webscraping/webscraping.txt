WEB SCRAPING LESSON 0.2
November 2016

- Who am I?
- Introduce others (Kim)
- At end of workshop: brief intro to SWC.

This is still experimental, in development.
Grateful for feedback.


CONTENTS

Part 1
- What is web scraping and why it is useful
- Using XPath to select elements on a web page
- Scraping manually using browser extensions and web services

Part 2
- Writing a spider to crawl a website
- Store and reuse extracted items
- Do's and Don'ts of web scraping


1 - INTRODUCTION - WHAT IS WEB SCRAPING

From Wikipedia:
	"Web scraping (web harvesting or web data extraction) is a computer software technique of extracting information from websites."

Closely related to web indexing (used by search engines like Google). They typically use tools called "bots" or "crawlers" to go through websites.
Difference:
	Indexing: going through ALL WEBSITES (unless blocked), store all content into database, follow ALL LINKS, index all stored data, repeat
	Scraping: go through SPECIFIC WEBSITES, follow SPECIFIED LINKS, extract unstructured information and put it in STRUCTURED FORM

Example:
http://www.parl.gc.ca/Parliamentarians/en/members

	Difference between UNSTRUCTURED and STRUCTURED data
	
	For humans, unstructured. We recognize names, provinces, political party, etc.
	But a machine doesn't, unless you tell it what is what.
	
	Computers need LABELS.
	This is a good website, because data is actually structured. 
		View Source. DIVs
		Export as XML/CSV. -> STRUCTURED DATA

Compare with
https://www.parliament.uk/mps-lords-and-offices/mps/

	As humans, we recognize names, parties, etc.
	
	But this data is not as nicely structured.
		View Source. Table

Say we want a spreadsheet with all British parliament members. How to extract this data? SCRAPING.


2 - USING XPATH TO SELECT DATA ON A WEB PAGE

XPath (which stands for XML Path Language) is an EXPRESSION LANGUAGE used to specify parts of an XML document.

	XML and HTML are both MARKUP LANGUAGES. Use some structure to separate elements.
	HTML is actually a special case of XML.
	
	
	XML basic syntax
	
	<catfood>
		<manufacturer>Purina</manufacturer>
		<address> 12 Cat Way, Boise, Idaho, 21341</address>
		<date>2019-10-01</date>
	</catfood>

	Nodes - Elements, attributes, text nodes
	Opening and closing tags around elements
	Nesting
	Attribute values 

XPath is a language to navigate an XML document to reach a particular element or group of elements.

Look at source code of http://labs.timtom.ch/library-webscraping/02-xpath/

There is a title element: <title>Selecting content on a web page with XPath</title>

Using the console, we can try out XPath queries using built-in JQuery functions:

$x("/html/head/title/text()")

		^-- explain that path

================================================================================================	
CHALLENGE

	Write an XPath query that selects the “Introduction” title above and try running it in the console.
		
================================================================================================			


Look at the HTML node tree.

Everything is a node
- the entire document is a node
- all HTML elements are a node
- text inside elements are a node

They have HIERARCHICAL RELATIONSHIPS to each other
- parent/child/sibling
- sequence of connections between nodes is called a path

Top node is "root node"


Example, select all blockquote nodes on this page:

$x("/html/body/div/blockquote")

We can use // to skip levels:

$x("//blockquote") 

================================================================================================	
CHALLENGE

	Why is the second array longer than the first?
	
		The first one only selects blockquotes that are directly under the first div element.
		The second query selects all blockquotes, regardless of where they are (also nested ones)
================================================================================================			

Use the class attribute to filter down elements:

$x("//blockquote[@class='challenge']")

		^-- [] notation is a PREDICATE, it means "select blockquote elements WHERE this condition is met"
		
================================================================================================	
CHALLENGE (easy)

	Select the Introduction title by ID.
	
		$x("/html/body/div/h1[@id='introduction']")
================================================================================================

================================================================================================	
CHALLENGE (harder)

	Select this challenge box.
	
		$x("//h2[@id = 'select-this-challenge-box']/..")[0]
================================================================================================	



3 - MANUALLY SCRAPE DATA USING BROWSER EXTENSIONS

There are different options to scrape data:

- Manually (copy paste). Sometimes the best/fastest way. Humans are good at structuring information. We know what an address looks like, etc.
	Good for relatively small amount of data, sometimes only solution if no structural clues.

- Semi-manually, using tools such as browser extensions to indicate what data needs to be scraped.
	Good when data is somehow structured, e.g. table rows, titles in bold, etc.
	
Examples: Chrome Scraper extension
		(try ffscrap in Firefox)
	
Going back to British parliament example: https://www.parliament.uk/mps-lords-and-offices/mps/
	- Select first row of table
	- Right click -> Scrape similar
	
Uses XPath selectors -> cf XPath lesson
	
We can try this out
	$x("//tbody/tr[td]")
					^-- remember predicates? What does this mean?
							Select TR elements that contain a TD element.
	
Column selectors are also XPath expressions.
	
	Remember that XPath queries are always relative to context. First Selector sets context.
	We can try it out by typing
	
	$x("//tbody/tr[td]/*[1]")
	
		Export to Google Docs or clipboard/TSV.
		Can be cleaned up further using OpenRefine (e.g. separate first, last name, party)
	
Try copy-pasting results into text editor (TSV data)
	Lots of white space
	
Replace Columns with 
	
normalize-space(*[1])
normalize-space(*[2])
	

================================================================================================	
CHALLENGE 

	Scrape the list of Ontario MPPs: www.ontla.on.ca/web/members/members_current.do?locale=en
	
	Once you get the name and riding, try adding a third column with the URL for the detail page
	(under the name of politician).
	
		Add column
		*[1]/a/@href
		
		Part of the URL is missing. Use concat() function
		concat('http://www.ontla.on.ca/web/members/',*[1]/a/@href)
================================================================================================	

Custom XPATH
		
Sometimes not as clear cut. Addresses of Canadian MPs:
	http://www.parl.gc.ca/Parliamentarians/en/members/addresses
	
	- Select 1st MP, Scrape similar
	- Not what we want

Useful to inspect web page to find structure.

	- Chrome or Firefox -> inspect element
	- MPs are inside ULs
	
Try changing Xpath with
	
//body/div[1]/div/ul
	
Click "Scrape"
	
Better, but still unstructured. We see that name, addresses separated by LI elements.
	
	In Columns, replace . by ./li and rename column.
	
	Add new columns:
	./li[1] -> Name
	./li[2] -> Hill Office
	./li[3] -> Constituency
		
	Etc. Then export and clean up using OpenRefine.
	
	
================================================================================================	
CHALLENGE

	Keep working on the example above to add a column for the Hill Office phone number and fax number for each MP.
	
		./li[2]/span[3] -> Hill Office Phone
		./li[2]/span[5] -> Hill Office Fax
	
	
================================================================================================


CHAINING SCRAPERS

Going back to the Ontario MPP example.

So we have a list of MPPs, which each have an URL to a detail page.
What if we want a spreadsheet that includes some of the information from that detail page.

We can write a first scraper to get the URLs (we did so before)
Then a second one for the detailed information.

Scraper cannot do that. We need to use a bit more advanced tools.
Next week we will see how to build our own scraper using Python, but now we can use
import.io

	Free plan -> max 500 URLs per month
		(I have a demo Premium plan)
		Watch out when testing so as not to blow the limit.
		
Has a nice WYSIWYG like interface for scraping

Try loading the URL for the MPP list
(you are welcome to use another example, e.g. other province)

http://www.ontla.on.ca/web/members/members_current.do?locale=en

Does a moderately good job understanding that there's a table on that page and that we might be interested
in getting that data out.

It actually captures HTML for the name column. URL is embedded. What if we want this in a separate column.

Add column, select name, include HTML.

Then in table view, select 
	Output HTML
	
	Write a Regular Expression to extract the href bit:
	
	href="(.*)"
			^--- will capture what's in the parentheses
			we get what's captured by using the $1 variable in the second fields
	
	$1
	
		Remember that those URLs are relative? Missing the beginning.
		We can easily concatenate:
		
	http://www.ontla.on.ca/web/members/$1
	
	
Then try it out!

Click RUN URL

Preview -> 105 lines


This data can be easily downloaded.


OK now for the 2nd step

We want to use those URLs to harvest data from pages like http://www.ontla.on.ca/web/members/members_detail.do?locale=en&ID=7085

Let's create a second import.io scraper using that URL

Select the data we are interested in. Say, the email address, the current title, the political party.

	Might work for one, what about others? Can we be sure the data is at the same place?
	
	Let's try it out. import.io allows us to run a scraper on multiple URLs. Let's put 2-3 in that box.
	
	Run, preview.
	
	We see that some data has not been captured correctly.
	
	Let's investigate.
	
Show source / inspect. Use XPath to query elements

Email:

$x("//div[@class='mppcontact']//div[@class='email']")

Party:

It's actually after the second H2 element.

$x("//div[@class='mppinfoblock']/h2[2]/following-sibling::p")

Try it out on multiple URLs. Once we are confident, we can chain scrapers.

Use the output of one scraper to drive the other.


================================================================================================	
CHALLENGE

Work on your own example. E.g. other province.

Try selecting different data. Using XPath.

Chain scrapers.
================================================================================================	



Work on your own, ask questions. We can hang around a bit longer.




END OF PART 1
Mention SWC




4 - WRITING WEB SPIDERS - using Scrapy

Verify that Scrapy 1.1.0 is installed
	Type scrapy version in Terminal

	Requires Python 2.7+ or 3.4+
	Should work in either environment


EXPLAIN EXAMPLE
http://www.collegesinstitutes.ca/our-members/member-directory/


We will work in two steps, first gather all URLs
Then extract data for each college

	
Initialize Scrapy project
	Navigate to where you want to create the project
	Type scrapy startproject cancolleges

	This creates the following structure:
	
	cancolleges/
	    scrapy.cfg            # deploy configuration file

	    cancolleges/          # project's Python module, you'll import your code from here
	        __init__.py

	        items.py          # project items file

	        pipelines.py      # project pipelines file

	        settings.py       # project settings file

	        spiders/          # a directory where you'll later put your spiders
	            __init__.py
	            ...


DEFINING ITEMS
	This is where the data extracted will be stored.
	Items work like Python dictionaries.
	
	Remember that we will start by gathering all the URLs for the college pages.
	Let's start by defining an Item to store the College URLs.
	
	Edit cancolleges/cancolleges/items.py
	
	(You will see Scrapy has pre-populated this file)
	
	import scrapy

	class CancollegesItem(scrapy.Item):
	    # define the fields for your item here like:
	    # name = scrapy.Field()
	    name = scrapy.Field()
	    link = scrapy.Field()


DEFINING A SPIDER
	Spiders are the business end of the scraper. It's the bit of code that combs through a website and harvest data.
	They define an initial list of URLs to download, how to follow links, and how to parse the contents of pages to extract items.
	
	Create a new file called colleges_spider.py under cancolleges/cancolleges/spiders
	with
	
	import scrapy

	class CollegeSpider(scrapy.Spider):
	    name = "colleges"  # Identifies the scraper, has to be UNIQUE but does not need to be same name as spider filename
	
		# allowed_domains = ["collegesinstitutes.ca"] ### REAL URLS
	    # start_urls = ["http://www.collegesinstitutes.ca/our-members/member-directory/"]
	
	    allowed_domains = ["labs.timtom.ch"]
	    start_urls = ["http://labs.timtom.ch/swc-teaching-notes/webscraping/data/www.collegesinstitutes.ca/our-members/member-directory/index.html"]
		
	    def parse(self, response):
			# a method of the spider, which will be called with the downloaded 
			# Response object of each start URL. The response is passed to the method as the first and only argument.
	        print response.text

START CRAWLING
	
	Try it out by moving to project's top level directory and type
		scrapy crawl colleges
						^-- name of the spider we just created
	
		Should return a bunch of text, including the HTML content of the page we just scraped.

	This is not very convenient, so try instead
	
	import scrapy

	class CollegeSpider(scrapy.Spider):
	    name = "colleges"
	   	allowed_domains = ["labs.timtom.ch"]
	    start_urls = ["http://labs.timtom.ch/swc-teaching-notes/webscraping/data/www.collegesinstitutes.ca/our-members/member-directory/index.html"]
	
	    def parse(self, response):
	        with open("test.html", 'wb') as file:
	            file.write(response.body)

	Should create file "test.html". Look inside this file.
	

OK, so how do we now access the URLs we're interested in?
Let's start by going back to the website and use the Inspect tool to find the elements we're looking for.

We need to use "Selectors" to get to the elements we need. Scrapy uses XPath (or CSS) selectors for this.

Refresher:
Here are some examples of XPath expressions and their meanings:
- /html/head/title: selects the <title> element, inside the <head> element of an HTML document
- /html/head/title/text(): selects the text inside the aforementioned <title> element.
- //td: selects all the <td> elements
- //div[@class="mine"]: selects all div elements which contain an attribute class="mine"


In Chrome, we can use the console to try out XPath queries:
> $x('//*[@class="facetwp-results"]')

	This actually only works if there is only one class. More general answer:
	https://stackoverflow.com/questions/8808921/selecting-a-css-class-with-xpath


Drilling down:
> $x('//*[@class="facetwp-results"]/li/a/@href')

Once we found what we're looking for, let's edit the Spider accordingly:

Scrapy Selectors support different methods:
- xpath(): returns a list of selectors, each of which represents the nodes selected by the xpath expression given as argument.
		This is what we'll use
- css(): returns a list of selectors, each of which represents the nodes selected by the CSS expression given as argument.
- extract(): returns a unicode string with the selected data.
- re(): returns a list of unicode strings extracted by applying the regular expression given as argument.



	import scrapy

	class CollegeSpider(scrapy.Spider):
	    name = "colleges"
	    allowed_domains = ["labs.timtom.ch"]
	    start_urls = ["http://labs.timtom.ch/swc-teaching-notes/webscraping/data/www.collegesinstitutes.ca/our-members/member-directory/index.html"]
		
	    def parse(self, response):
	        for url in response.xpath('//*[@class="facetwp-results"]/li/a/@href').extract():
	            print url

Looking good.

OK, so we now have our URLs, how do we get further?

Let's have a look at the first college page.
What information are we interested in extracting?

- Name
- Website
- Address
- Telephone

================================================================================================
CHALLENGE
What are the XPath queries to extract the above details?
Hint: keep using the Inspect tool + console trick

$x('//*[@class="page-title"]')
$x('//*[@class="mem-contact"]/p[2]//a/@href')
$x('//*[@class="mem-contact"]/p[1]')
$x('//*[@class="mem-contact"]/p[2]/node()[2]') OR $x('//*[@class="mem-contact"]/p[2]/text()[1]')
================================================================================================


Other trick, we can also use Scrapy in shell mode to try out XPath selectors:
(actually an iPython shell)

$ scrapy shell "http://www.collegesinstitutes.ca/members/alberta-college-of-art-and-design/"
In [1]: reponse.body
In [2]: response.xpath('//*[@class="page-title"]')
	Whoops, is that what we want?
In [3]: response.xpath('//*[@class="page-title"]/text()').extract()

Etc.

To exit: CTRL-D (or CTRL-Z in Windows?)


How do we bring this back together?

Remember the CanCollegesItem we defined?

	import scrapy
	from cancolleges.items import CancollegesItem # We need this so that Python knows about the Item

	class CollegeSpider(scrapy.Spider):
	    name = "colleges"
	    allowed_domains = ["collegesinstitutes.ca"]
	    start_urls = ["http://www.collegesinstitutes.ca/our-members/member-directory/"]

	    def parse(self, response):
	        for url in response.xpath('//*[@class="facetwp-results"]/li/a/@href').extract():
	            yield scrapy.Request(url, callback=self.get_details) # Callback to the get_details() method

	    def get_details(self, response):
	        item = CancollegesItem()
	        item['name'] = response.xpath('//*[@class="page-title"]/text()').extract()
		    item['link'] = response.xpath('//*[@class="mem-contact"]/p[2]//a/@href').extract()
	        yield item # Return the item

yield is like return, but for potentially large datasets. Basically when you don't know from the beginning
how big the dataset will be.
	Also:
	return implies that the function is returning control of execution to the point where the function was called. 
	yield, however, implies that the transfer of control is temporary and voluntary, and our function expects to regain it in the future.
	From http://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/


================================================================================================
CHALLENGE
Add the other elements to the Spider.
Remember to edit the Item definition to allow for all fields to be taken care of.

Advanced:
Check http://www.collegesinstitutes.ca/members/aurora-college/
Look at column with enrollment numbers. Several colleges have these. Can you incorporate them in your spider?
Don't forget to update the Item accordingly
================================================================================================


Running the spider above returns CancollegesItem objects.
What if we want this data in another, more usable format?

Feed exports > very powerful function of Scrapy

Just type
$ scrapy crawl colleges -o items.csv

or
$ scrapy crawl colleges -o items.json

Also works with XML, Pickle, etc.
http://doc.scrapy.org/en/latest/topics/feed-exports.html#topics-feed-exports

MAGIC!!

================================================================================================
ASK

Would you like to work on your own project
OR
Should I keep explain things like how to run spiders on cloud platforms
================================================================================================



CHECK IF ENOUGH TIME FOR LEGAL ASPECTS


Going further:
- Pipelines
	Can be used to check validity of returned data
	http://doc.scrapy.org/en/latest/topics/item-pipeline.html



CLOUD-BASED SCRAPING
- ScrapingHub, Scrapy Cloud
- Morph.io


Example deploy on ScrapingHub

- Login/Register on https://app.scrapinghub.com/
	Use Google account


- If not done already, run
	$ pip install shub

- $ shub login
	Provide your API as found on https://app.scrapinghub.com/account/apikey
	This writes the API key into a local file: less ~/.scrapinghub.yml
	Not specific to that repository 
	shub logout to remove it

- Create a new project, note its ID
- cd to the root of a scrapy project directory
	
- $ shub deploy
	Provide the ID of the recently created project
	
- From the web app, run spider
	Arguments can be provided
	
- Items are found under the Items tab
	Free project: one concurrent spider, max 2.5GB of data storage, data retention 7 days
	Items can be downloaded individually or in batch (big green Items button on top)
	Can also be accessed through API calls
		http://doc.scrapinghub.com/api/overview.html
		e.g. curl -u 1e2490bfc15d4e6089e4b842364b5cd1: "https://storage.scrapinghub.com/items/85589/1/1?format=xml"
		

- Jobs can be scheduled, etc.

They have a GUI (work in progress) to define elements and build spiders: Portia


CHECK IF ENOUGH TIME FOR LEGAL ASPECTS



ALTERNATIVE: MORPH.IO
- Port of ScraperWiki Classic
- Open source
- Project supported by the OpenAustralia Foundation
- Can be run locally, but also on cloud platform morph.io "Heroku for scrapers"- very convenient - works with GitHub
	requires a working install of git
	requires ruby or python + several libraries for local running
- For this reason, not chosen for this workshop
- Platform runs the scraper and also stores data
	Data can easily be shared, downloaded in structured format and accessed through dedicated API
- Demo if enough time


CHECK IF ENOUGH TIME FOR LEGAL ASPECTS



SCRAPE DATA FROM PDFs

Use Tabula
	- Free, open source software
	- Available for Mac, Windows, Linux
	- Runs in browser (much like OpenRefine)
	https://github.com/tabulapdf/tabula


This example uses a library that converts PDF to XML, then does the extraction.

http://www.bl.uk/reshelp/atyourdesk/docsupply/help/replycodes/dirlibcodes/
https://morph.io/ostephens/british_library_directory_of_library_codes
	British Library maintains list of library codes for its Document Supply service.
	List of codes in a PDF file -> UNSTRUCTURED
	
	




DOES and DONTS - LEGAL ASPECTS

Can be illegal, check terms of use of target websites.
Generally
	Web scraping = tool.
		"using a computer to read a website"
	If data is publicly available, it's OK to scrape it, provided it does not brake the website.
	Sharing data is the problematic bit.
	Usually OK if public data (e.g. government data)

Usually OK for personal consumption. Good practice to contact data owners/curators if aim to use it for institutional purposes
- You might receive data already structured
- Clarifies right to share extracted data

Unfortunatly, not all data curators will be sensible to this.

Don't download copies of documents that are clearly not public (c.f. SciHub...)
Content theft is illegal
Generally safe if not for commercial intent.
	Case law evolving in the US, cases where scraping was declared illegal involved commercial aspects
		e.g. competitor to eBay automatically placing bets
		e.g. extracting pricing data to set own prices
		"trespass to chattel"

Harvesting personal information (e.g. email addresses, cf Australia's Spam Act of 2013) might be illegal. Be careful, especially if sharing extracted data.


BUT even if legal, badly programmed scrapers can also bring down a website (Denial of Service attack).
Test your code on small dataset first.


Good practice:
- if you extract PUBLIC data, share it
	- code on GitHub
	- data on GitHub / datahub.io etc.
	- or use morph.io
	
IF YOU PUBLISH DATA, PROVIDE IT IN STRUCTURED FORM TO BEGIN WITH.
	Web scraping is a "tool of last resort" when data curators fail to provide structured data.


GOING FURTHER
HTTP-authentication is supported by many frameworks, allows for scraping of protected data
	Warning about legal aspects!
There are frameworks that reproduce the workings of a web browser (e.g. PhantomJS)





	



References:
- https://en.wikipedia.org/wiki/Web_scraping
- http://blog.rubyroidlabs.com/2016/04/web-scraping-1/
- http://schoolofdata.org/handbook/courses/scraping/
- http://schoolofdata.org/handbook/recipes/scraper-extension-for-chrome/
- http://doc.scrapy.org/en/1.1/intro/tutorial.html


