WEB SCRAPING LESSON 0.3
February 2017

- Who am I?
- Introduce others (Kim)
- At end of workshop: brief intro to SWC.

This is still experimental, in development.
Grateful for feedback.


CONTENTS

Part 1
- What is web scraping and why it is useful
- Using XPath to select elements on a web page
- Scraping manually using browser extensions and web services

Part 2
- Writing a spider to crawl a website
- Store and reuse extracted items
- Do's and Don'ts of web scraping


1 - INTRODUCTION - WHAT IS WEB SCRAPING

From Wikipedia:
	"Web scraping (web harvesting or web data extraction) is a computer software technique of extracting information from websites."

Closely related to web indexing (used by search engines like Google). They typically use tools called "bots" or "crawlers" to go through websites.
Difference:
	Indexing: going through ALL WEBSITES (unless blocked), store all content into database, follow ALL LINKS, index all stored data, repeat
	Scraping: go through SPECIFIC WEBSITES, follow SPECIFIED LINKS, extract unstructured information and put it in STRUCTURED FORM

Example:
http://www.parl.gc.ca/Parliamentarians/en/members

	Difference between UNSTRUCTURED and STRUCTURED data

	For humans, unstructured. We recognize names, provinces, political party, etc.
	But a machine doesn't, unless you tell it what is what.

	Computers need LABELS.
	This is a good website, because data is actually structured.
		View Source. DIVs
		Export as XML/CSV. -> STRUCTURED DATA

Compare with
https://www.parliament.uk/mps-lords-and-offices/mps/

	As humans, we recognize names, parties, etc.

	But this data is not as nicely structured.
		View Source. Table

Say we want a spreadsheet with all British parliament members. How to extract this data? SCRAPING.


2 - USING XPATH TO SELECT DATA ON A WEB PAGE

XPath (which stands for XML Path Language) is an EXPRESSION LANGUAGE used to specify parts of an XML document.

	XML and HTML are both MARKUP LANGUAGES. Use some structure to separate elements.
	HTML is actually a special case of XML.


	XML basic syntax

	<catfood>
		<manufacturer>Purina</manufacturer>
		<address> 12 Cat Way, Boise, Idaho, 21341</address>
		<date>2019-10-01</date>
	</catfood>

	Nodes - Elements, attributes, text nodes
	Opening and closing tags around elements
	Nesting
	Attribute values

XPath is a language to navigate an XML document to reach a particular element or group of elements.

Look at source code of http://labs.timtom.ch/library-webscraping/02-xpath/

There is a title element: <title>Selecting content on a web page with XPath</title>

Using the console, we can try out XPath queries using built-in JQuery functions:

$x("/html/head/title/text()")

		^-- explain that path

================================================================================================
CHALLENGE

	Write an XPath query that selects the “Introduction” title above and try running it in the console.

================================================================================================


Look at the HTML node tree.

Everything is a node
- the entire document is a node
- all HTML elements are a node
- text inside elements are a node

They have HIERARCHICAL RELATIONSHIPS to each other
- parent/child/sibling
- sequence of connections between nodes is called a path

Top node is "root node"


Example, select all blockquote nodes on this page:

$x("/html/body/div/blockquote")

We can use // to skip levels:

$x("//blockquote")

================================================================================================
CHALLENGE

	Why is the second array longer than the first?

		The first one only selects blockquotes that are directly under the first div element.
		The second query selects all blockquotes, regardless of where they are (also nested ones)
================================================================================================

Use the class attribute to filter down elements:

$x("//blockquote[@class='challenge']")

		^-- [] notation is a PREDICATE, it means "select blockquote elements WHERE this condition is met"

================================================================================================
CHALLENGE (easy)

	Select the Introduction title by ID.

		$x("/html/body/div/h1[@id='introduction']")
================================================================================================

================================================================================================
CHALLENGE (harder)

	Select this challenge box.

		$x("//h2[@id = 'select-this-challenge-box']/..")[0]
================================================================================================



3 - MANUALLY SCRAPE DATA USING BROWSER EXTENSIONS

There are different options to scrape data:

- Manually (copy paste). Sometimes the best/fastest way. Humans are good at structuring information. We know what an address looks like, etc.
	Good for relatively small amount of data, sometimes only solution if no structural clues.

- Semi-manually, using tools such as browser extensions to indicate what data needs to be scraped.
	Good when data is somehow structured, e.g. table rows, titles in bold, etc.

Examples: Chrome Scraper extension
		(try ffscrap in Firefox)

Going back to British parliament example: https://www.parliament.uk/mps-lords-and-offices/mps/
	- Select first row of table
	- Right click -> Scrape similar

Uses XPath selectors -> cf XPath lesson

We can try this out
	$x("//tbody/tr[td]")
					^-- remember predicates? What does this mean?
							Select TR elements that contain a TD element.

Column selectors are also XPath expressions.

	Remember that XPath queries are always relative to context. First Selector sets context.
	We can try it out by typing

	$x("//tbody/tr[td]/*[1]")

		Export to Google Docs or clipboard/TSV.
		Can be cleaned up further using OpenRefine (e.g. separate first, last name, party)

Try copy-pasting results into text editor (TSV data)
	Lots of white space

Replace Columns with

normalize-space(*[1])
normalize-space(*[2])


================================================================================================
CHALLENGE

	Scrape the list of Ontario MPPs: www.ontla.on.ca/web/members/members_current.do?locale=en

	Once you get the name and riding, try adding a third column with the URL for the detail page
	(under the name of politician).

		Add column
		*[1]/a/@href

		Part of the URL is missing. Use concat() function
		concat('http://www.ontla.on.ca/web/members/',*[1]/a/@href)
================================================================================================

Custom XPATH

Sometimes not as clear cut. Addresses of Canadian MPs:
	http://www.parl.gc.ca/Parliamentarians/en/members/addresses

	- Select 1st MP, Scrape similar
	- Not what we want

Useful to inspect web page to find structure.

	- Chrome or Firefox -> inspect element
	- MPs are inside ULs

Try changing Xpath with

//body/div[1]/div/ul

Click "Scrape"

Better, but still unstructured. We see that name, addresses separated by LI elements.

	In Columns, replace . by ./li and rename column.

	Add new columns:
	./li[1] -> Name
	./li[2] -> Hill Office
	./li[3] -> Constituency

	Etc. Then export and clean up using OpenRefine.


================================================================================================
CHALLENGE

	Keep working on the example above to add a column for the Hill Office phone number and fax number for each MP.

		./li[2]/span[3] -> Hill Office Phone
		./li[2]/span[5] -> Hill Office Fax


================================================================================================



END OF PART 1
Mention SWC




4 - WRITING WEB SPIDERS - using Scrapy

(source activate scraping)


Verify that Scrapy 1.3.2 is installed (though should also work with older version)
  Use conda install -c conda-forge scrapy to get latest version
	Type scrapy version in Terminal

	Requires Python 2.7+ or 3.4+
	Should work in either environment

	If you are familiar with environments, it might be helpful to install it in an environment
	e.g.
	conda create --name scrapy python=3
	source activate scrapy
	conda install -c conda-forge scrapy
	(to get the latest version)


EXPLAIN EXAMPLE
http://www.ontla.on.ca/web/members/members_current.do?locale=en


We will work in two steps, first gather all URLs
Then extract data for each minister


Initialize Scrapy project
	Navigate to where you want to create the project
	Type scrapy startproject ontariompps

	This creates the following structure:

	ontariompps/
	    scrapy.cfg            # deploy configuration file

	    ontariompps/          # project's Python module, you'll import your code from here
	        __init__.py

	        items.py          # project items file

	        pipelines.py      # project pipelines file

	        settings.py       # project settings file

	        spiders/          # a directory where you'll later put your spiders
	            __init__.py
	            ...

(yes, confusingly, scrapy makes a ontariompps folder within a ontariompps folder...)


DEFINING A SPIDER
Spiders are the business end of the scraper. It's the bit of code that combs through a website and harvest data.
They define an initial list of URLs to download, how to follow links, and how to parse the contents of pages to extract items.

Create a new spider with

scrapy genspider mppaddresses www.ontla.on.ca/web/members/members_current.do?locale=en

WARNING do not add http:// because Scrapy is still a bit stupid.

Check out the results: in spiders directory:

	import scrapy

	class MppaddressesSpider(scrapy.Spider):
	    name = "mppaddresses"  # The name of this spider
	
		# The allowed domain and the URLs where the spider should start crawling:
	    allowed_domains = ["www.ontla.on.ca/web/members/members_current.do?locale=en"]
	    start_urls = ['http://www.ontla.on.ca/web/members/members_current.do?locale=en/']

		# And a 'parse' function, which is the main method of the spider. The content of the scraped
		# URL is passed on as the 'response' object:
	    def parse(self, response):
	        pass

(ask if everyone is familiar with OBJECT-ORIENTED PROGRAMMING. If not, spend some time explaining.)

================================================================================================
CHALLENGE
	Look at allowed_domains. What will happen?
	Is this what we want? How should we edit this?
================================================================================================




START CRAWLING

Try it out by moving to project's top level directory and type
	scrapy crawl mppaddresses
				       ^-- name of the spider we just created

Should return some debug output, look for code 200 (this means OK).

Try now editing the spider:

	import scrapy


	class MppaddressesSpider(scrapy.Spider):
	    name = "mppaddresses"
	    allowed_domains = ["www.ontla.on.ca"]
	    start_urls = ['http://www.ontla.on.ca/web/members/members_current.do?locale=en/']

	    def parse(self, response):
	        with open("test.html", 'wb') as file:
	            file.write(response.body)


Crawl again. This time it should have created test.html file with contents.


OK, so how do we now access the URLs we're interested in?
Let's start by going back to the website and use the Inspect tool to find the elements we're looking for.

We need to use "Selectors" to get to the elements we need. Scrapy uses XPath (or CSS) selectors for this.

Refresher:
Here are some examples of XPath expressions and their meanings:
- /html/head/title: selects the <title> element, inside the <head> element of an HTML document
- /html/head/title/text(): selects the text inside the aforementioned <title> element.
- //td: selects all the <td> elements
- //div[@class="mine"]: selects all div elements which contain an attribute class="mine"


In Chrome, we can use the console to try out XPath queries:
> $x("//td[@class='mppcell']")

	This actually only works if there is only one class. More general answer:
	> `//*[contains(concat(" ", normalize-space(@class), " "), " mppcell ")]`


Drilling down:
> $x("//td[@class='mppcell']/a/@href")

But now that we have started to play with Scrapy, let's try something else out.

Type
	scrapy shell http://www.ontla.on.ca/web/members/members_current.do?locale=en/

This allows us to test Python/Scrapy functions live on the scraped page.

	>>> response.xpath("//td[@class='mppcell']/a/@href")
	
Point out that the result is an ARRAY of SELECTORS
We are interested in the actual content pointed by the selectors. And just the first one for now:

	>>> response.xpath("//td[@class='mppcell']/a/@href").extract_first()

Looking in detail, we see the links are RELATIVE.
Will this be an issue?

	>>> testurl = response.xpath("//td[@class='mppcell']/a/@href").extract_first()
	>>> response.urljoin(testurl)

To exit: CTRL-D (or CTRL-Z in Windows?)


Now back to the spider to bring it all together:

	import scrapy

	class MppaddressesSpider(scrapy.Spider):
	    name = "mppaddresses"
	    allowed_domains = ["www.ontla.on.ca"]
		start\_urls = ['http://www.ontla.on.ca/web/members/members_current.do?locale=en/']

	    def parse(self, response):
	        for url in response.xpath("//*[@class='mppcell']/a/@href").extract():
	            print(response.urljoin(url))


Scrapy Selectors support different methods:
- xpath(): returns a list of selectors, each of which represents the nodes selected by the xpath expression given as argument.
		This is what we'll use
- css(): returns a list of selectors, each of which represents the nodes selected by the CSS expression given as argument.

- extract(): returns a unicode string with the selected data.
- extract_first()
- re(): returns a list of unicode strings extracted by applying the regular expression given as argument.
- re_first()

================================================================================================
CHALLENGE
	Why am I using extract() and not extract_first() ?
	Why the for loop?
================================================================================================


	scrapy crawl mppaddresses

Looking good.

OK, so we now have our URLs, how do we get further?


LIMIT
Good idea when debugging to limit to a subset. We've seen above that our code works for all URLs, going forward
let's only run it on 5 of them.

	import scrapy


	class MppaddressesSpider(scrapy.Spider):
	    name = "mppaddresses"
	    allowed_domains = ["www.ontla.on.ca"]
	    start_urls = ['http://www.ontla.on.ca/web/members/members_current.do?locale=en/']

	    def parse(self, response):
	        for url in response.xpath("//*[@class='mppcell']/a/@href").extract()[:5]:
	            print(response.urljoin(url))



RECURSIVE SCRAPING

Start by defining a get_details method and chain it:

import scrapy

class MppaddressesSpider(scrapy.Spider):
    name = "mppaddresses" # The name of this spider
    
    # The allowed domain and the URLs where the spider should start crawling:
    allowed_domains = ["www.ontla.on.ca"]
    start_urls = ['http://www.ontla.on.ca/web/members/members_current.do?locale=en/']

    def parse(self, response):
		# The main method of the spider. It scrapes the URL(s) specified in the
		# 'start_url' argument above. The content of the scraped URL is passed on
		# as the 'response' object.
        
		for url in response.xpath("//*[@class='mppcell']/a/@href").extract()[:5]:
			# This loops through all the URLs found inside an element of class 'mppcell'
			
			# Constructs an absolute URL by combining the response’s URL with a possible relative URL:
			full_url = response.urljoin(url)
			print("Found URL: "+full_url)
            
			# The following tells Scrapy to scrape the URL in the 'full_url' variable
			# and calls the 'get_details() method below with the content of this
			# URL:
			yield scrapy.Request(full_url, callback=self.get_details)
    
	def get_details(self, response):
		# This method is called on by the 'parse' method above. It scrapes the URLs
		# that have been extracted in the previous step.
		print("Visited URL: "+response.url)
		

EXPLAIN
- yield
- callback
- ASYNCHRONOUS REQUESTS



Let's have a look at the first detail page.
What information are we interested in extracting?

- Phone number(s)
- Email address(es)

Look at some examples. There are inconsistencies (not all numbers or emails are in the same location).
In real life we would write queries that take care of this, for now we will just harvest the 1st number.


================================================================================================
CHALLENGE
What are the XPath queries to extract the above details?
Hint: keep using the Inspect tool + console trick OR Scrapy shell

List of phone and fax numbers:
$x("//div[@class='phone']/text()")

List of email addresses:
$x("//div[@class='email']/a/text()")

================================================================================================

================================================================================================
CHALLENGE - LEVEL 2
Write a Regular Expression to grab the phone numbers.

scrapy shell "http://www.ontla.on.ca/web/members/members_detail.do?locale=en&ID=7085"
>>> response.xpath('//body').re(r'\d{3}-\d{3}-\d{4}')

================================================================================================


Use normalize-space to remove extra white space.



Putting it all together:

# -*- coding: utf-8 -*-
import scrapy


class MppaddressesSpider(scrapy.Spider):
    name = "mppaddresses" # The name of this spider
    
    # The allowed domain and the URLs where the spider should start crawling:
    allowed_domains = ["www.ontla.on.ca"]
    start_urls = ['http://www.ontla.on.ca/web/members/members_current.do?locale=en/']

    def parse(self, response):
		# The main method of the spider. It scrapes the URL(s) specified in the
        # 'start_url' argument above. The content of the scraped URL is passed on
		# as the 'response' object.
        for url in response.xpath("//*[@class='mppcell']/a/@href").extract()[:5]:
            # This loops through all the URLs found inside an element of class 'mppcell'
            
            # Constructs an absolute URL by combining the response’s URL with a possible relative URL:
            full_url = response.urljoin(url)
            print("Found URL: "+full_url)
            
            # The following tells Scrapy to scrape the URL in the 'full_url' variable
            # and calls the 'get_details() method below with the content of this
            # URL:
            yield scrapy.Request(full_url, callback=self.get_details)
    
    def get_details(self, response):
		# This method is called on by the 'parse' method above. It scrapes the URLs
        # that have been extracted in the previous step.
	    name_detail = response.xpath("normalize-space(//div[@class='mppdetails']/h1/text())").extract_first()
	    phone_detail = response.xpath("normalize-space(//div[@class='phone']/text())").extract_first()
	    email_detail = response.xpath("normalize-space(//div[@class='email']/a/text())").extract_first()
	    print("Found details: " + name_detail + ', ' + phone_detail + ', ' + email_detail)



YIELD:
yield is like return, but for potentially large datasets. Basically when you don't know from the beginning
how big the dataset will be.
Also:
	return implies that the function is returning control of execution to the point where the function was called.
	yield, however, implies that the transfer of control is temporary and voluntary, and our function expects to regain it in the future.
	From http://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/



DEFINING ITEMS
	This is where the data extracted will be stored.
	Items work like Python dictionaries.

(editing ontariompps/ontariompps/items.py)

	import scrapy

	class OntariomppsItem(scrapy.Item):
	    # define the fields for your item here like:
	    name = scrapy.Field()
	    phone = scrapy.Field()
		email = scrapy.Field()

Now use it in the spider:

import scrapy
from ontariompps.items import OntariomppsItem # We need this so that Python knows about the item object

class MppaddressesSpider(scrapy.Spider):
    name = "mppaddresses" # The name of this spider
    
    # The allowed domain and the URLs where the spider should start crawling:
    allowed_domains = ["www.ontla.on.ca"]
    start_urls = ['http://www.ontla.on.ca/web/members/members_current.do?locale=en/']

    def parse(self, response):
		# The main method of the spider. It scrapes the URL(s) specified in the
        # 'start_url' argument above. The content of the scraped URL is passed on
		# as the 'response' object.
        for url in response.xpath("//*[@class='mppcell']/a/@href").extract()[:5]:
            # This loops through all the URLs found inside an element of class 'mppcell'
            
            # Constructs an absolute URL by combining the response’s URL with a possible relative URL:
            full_url = response.urljoin(url)
            print("Found URL: "+full_url)
            
            # The following tells Scrapy to scrape the URL in the 'full_url' variable
            # and calls the 'get_details() method below with the content of this
            # URL:
            yield scrapy.Request(full_url, callback=self.get_details)
    
    def get_details(self, response):
		# This method is called on by the 'parse' method above. It scrapes the URLs
        # that have been extracted in the previous step.
        
        item = OntariomppsItem() # Creating a new Item object
        # Store scraped data into that item:
        item['name'] = response.xpath("normalize-space(//div[@class='mppdetails']/h1/text())").extract_first()
        item['phone'] = response.xpath("normalize-space(//div[@class='phone']/text())").extract_first()
        item['email'] = response.xpath("normalize-space(//div[@class='email']/a/text())").extract_first()
	    
        # Return that item to the main spider method:
        yield item


Running the spider above returns OntariomppsItem objects.
What if we want this data in another, more usable format?

Feed exports > very powerful function of Scrapy

Just type
$ scrapy crawl getemails -o items.csv

or
$ scrapy crawl getemails -o items.json

Also works with XML, Pickle, etc.
http://doc.scrapy.org/en/latest/topics/feed-exports.html#topics-feed-exports

MAGIC!!

================================================================================================
ASK

Would you like to work on your own project
OR
Should I keep explain things like how to run spiders on cloud platforms
================================================================================================



CHECK IF ENOUGH TIME FOR LEGAL ASPECTS


Going further:
- Pipelines
	Can be used to check validity of returned data
	http://doc.scrapy.org/en/latest/topics/item-pipeline.html



CLOUD-BASED SCRAPING
- ScrapingHub, Scrapy Cloud
- Morph.io


Example deploy on ScrapingHub

- Login/Register on https://app.scrapinghub.com/
	Use Google account


- If not done already, run
	$ pip install shub

- $ shub login
	Provide your API as found on https://app.scrapinghub.com/account/apikey
	This writes the API key into a local file: less ~/.scrapinghub.yml
	Not specific to that repository
	shub logout to remove it

- Create a new project, note its ID
- cd to the root of a scrapy project directory

- $ shub deploy
	Provide the ID of the recently created project

- From the web app, run spider
	Arguments can be provided

- Items are found under the Items tab
	Free project: one concurrent spider, max 2.5GB of data storage, data retention 7 days
	Items can be downloaded individually or in batch (big green Items button on top)
	Can also be accessed through API calls
		http://doc.scrapinghub.com/api/overview.html
		e.g. curl -u 1e2490bfc15d4e6089e4b842364b5cd1: "https://storage.scrapinghub.com/items/85589/1/1?format=xml"


- Jobs can be scheduled, etc.

They have a GUI (work in progress) to define elements and build spiders: Portia


CHECK IF ENOUGH TIME FOR LEGAL ASPECTS



ALTERNATIVE: MORPH.IO
- Port of ScraperWiki Classic
- Open source
- Project supported by the OpenAustralia Foundation
- Can be run locally, but also on cloud platform morph.io "Heroku for scrapers"- very convenient - works with GitHub
	requires a working install of git
	requires ruby or python + several libraries for local running
- For this reason, not chosen for this workshop
- Platform runs the scraper and also stores data
	Data can easily be shared, downloaded in structured format and accessed through dedicated API
- Demo if enough time


CHECK IF ENOUGH TIME FOR LEGAL ASPECTS



SCRAPE DATA FROM PDFs

Use Tabula
	- Free, open source software
	- Available for Mac, Windows, Linux
	- Runs in browser (much like OpenRefine)
	https://github.com/tabulapdf/tabula


This example uses a library that converts PDF to XML, then does the extraction.

http://www.bl.uk/reshelp/atyourdesk/docsupply/help/replycodes/dirlibcodes/
https://morph.io/ostephens/british_library_directory_of_library_codes
	British Library maintains list of library codes for its Document Supply service.
	List of codes in a PDF file -> UNSTRUCTURED






DOES and DONTS - LEGAL ASPECTS

Can be illegal, check terms of use of target websites.
Generally
	Web scraping = tool.
		"using a computer to read a website"
	If data is publicly available, it's OK to scrape it, provided it does not brake the website.
	Sharing data is the problematic bit.
	Usually OK if public data (e.g. government data)

Usually OK for personal consumption. Good practice to contact data owners/curators if aim to use it for institutional purposes
- You might receive data already structured
- Clarifies right to share extracted data

Unfortunatly, not all data curators will be sensible to this.

Don't download copies of documents that are clearly not public (c.f. SciHub...)
Content theft is illegal
Generally safe if not for commercial intent.
	Case law evolving in the US, cases where scraping was declared illegal involved commercial aspects
		e.g. competitor to eBay automatically placing bets
		e.g. extracting pricing data to set own prices
		"trespass to chattel"

Harvesting personal information (e.g. email addresses, cf Australia's Spam Act of 2013) might be illegal. Be careful, especially if sharing extracted data.


BUT even if legal, badly programmed scrapers can also bring down a website (Denial of Service attack).
Test your code on small dataset first.


Good practice:
- if you extract PUBLIC data, share it
	- code on GitHub
	- data on GitHub / datahub.io etc.
	- or use morph.io

IF YOU PUBLISH DATA, PROVIDE IT IN STRUCTURED FORM TO BEGIN WITH.
	Web scraping is a "tool of last resort" when data curators fail to provide structured data.


GOING FURTHER
HTTP-authentication is supported by many frameworks, allows for scraping of protected data
	Warning about legal aspects!
There are frameworks that reproduce the workings of a web browser (e.g. PhantomJS)









References:
- https://en.wikipedia.org/wiki/Web_scraping
- http://blog.rubyroidlabs.com/2016/04/web-scraping-1/
- http://schoolofdata.org/handbook/courses/scraping/
- http://schoolofdata.org/handbook/recipes/scraper-extension-for-chrome/
- http://doc.scrapy.org/en/1.1/intro/tutorial.html
